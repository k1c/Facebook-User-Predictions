{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0892410fe811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mignite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_supervised_trainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_supervised_evaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mignite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from ignite.engine import create_supervised_trainer, create_supervised_evaluator, Events\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch import tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_df_train = pd.read_csv('../../new_data/Train/Relation/Relation.csv').drop(columns=['Unnamed: 0'])\n",
    "relation_df_test = pd.read_csv('../../new_data/Public_Test/Relation/Relation.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "train_set_like_ids = set(relation_df_train['like_id'])\n",
    "test_set_like_ids = set(relation_df_test['like_id'])\n",
    "common_like_ids = set(sorted(train_set_like_ids.intersection(test_set_like_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 536204 training page ids\n",
      "There are 37073 test page ids\n",
      "We have 23581 page ids that exist both in the train set and the validation set\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} training page ids\".format(len(train_set_like_ids)))\n",
    "print(\"There are {} test page ids\".format(len(test_set_like_ids)))\n",
    "print(\"We have {} page ids that exist both in the train set and the validation set\".format(len(common_like_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_constructable_vectors(df):\n",
    "    count = 0\n",
    "    for userid in df['userid'].unique():\n",
    "        user_likes = df[df['userid'] == userid]['like_id']\n",
    "        if set(user_likes).intersection(common_like_ids):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are able to construct a train vector for 90.46315789473684% of users\n",
      "We are able to construct a test vector for 95.20958083832335% of users\n"
     ]
    }
   ],
   "source": [
    "train_constructable_vectors = get_num_constructable_vectors(relation_df_train)\n",
    "test_constructable_vectors = get_num_constructable_vectors(relation_df_test)\n",
    "print(\"Using only the common ids:\")\n",
    "print(\"We are able to construct a train vector for {}% of users\".format(100*train_constructable_vectors/len(relation_df_train['userid'].unique())))\n",
    "print(\"We are able to construct a test vector for {}% of users\".format(100*test_constructable_vectors/len(relation_df_test['userid'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct vectors for each user, of lenght len(common_ids)+1 where 0 in the vector represents he liked that page and 1 that he did. If all of the values end up as 0 we set the last index as 1 meaning the user liked a different page (one of the other 500.000 ones we left out)\n",
    "Whenever there is a user that did not like one of the common ids, we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_likes(dataframe):\n",
    "    return dataframe['like_id']\n",
    "\n",
    "def get_user_like_vector(user_likes, common_like_ids):\n",
    "    user_like_vector = np.zeros(len(common_like_ids)+1)\n",
    "    for like_id in user_likes:\n",
    "        page_liked_index = np.argwhere(common_like_ids == like_id)\n",
    "        if page_liked_index.size > 0:\n",
    "            user_like_vector[page_liked_index[0][0]] = 1\n",
    "\n",
    "def get_features(dataframe, common_like_ids):\n",
    "    user_ids = df['userid'].unique()\n",
    "    like_feature_vectors = np.array([\n",
    "        get_user_like_vector(user_likes, common_like_ids)\n",
    "        for user_likes in dataframe['like_id']\n",
    "    ])\n",
    "    get_user_like_vector\n",
    "            \n",
    "user_ids, like_feature_vector = \n",
    "user_ids = train_set_like_ids['userid'].unique()\n",
    "for user_id in user_ids:\n",
    "    \n",
    "user_likes = train_set_like_ids['userid']\n",
    "\n",
    "user_like_vector = np.sum([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "import torch.tensor\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os.path.join\n",
    "\n",
    "class FBRelationV2PreprocessedDataset(Dataset):\n",
    "    def __init__(self, features: torch.tensor, labels: torch.tensor or None):\n",
    "        if labels is not None:\n",
    "            assert len(features) == len(labels)\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, int]:\n",
    "        if self.labels is None:\n",
    "            return self.features[idx]\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "class BasicNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simple neural network architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, hidden_layer_sizes, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(num_inputs, hidden_layer_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_layer_sizes[1], hidden_layer_sizes[2])\n",
    "        self.fc4 = nn.Linear(hidden_layer_sizes[2], num_outputs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = F.relu(self.fc1(input.float()))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.relu(self.fc3(h))\n",
    "        out = F.relu(self.fc4(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class RelationV2AgeEstimator():\n",
    "    def __init__(self):\n",
    "        # * 23581 common like ids in training and validation\n",
    "        #   + 1 for \"other\"\n",
    "        # * Ages 1-112\n",
    "        self.neural_net = BasicNN(23582, [10000, 5000, 2500], 112)\n",
    "        self.batch_size = 10\n",
    "        self.learning_rate = 0.01\n",
    "        self.num_epochs = 100\n",
    "        self.predictions = []\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            features.reshape(-1, 23582),\n",
    "            np.array([\n",
    "                # Converting an age to one-hot. Example: '3' -> [0, 0, 1, 0, ...]\n",
    "                np.eye(112)[np.array([age-1])].tolist()[0]\n",
    "                for age in labels\n",
    "            ]),\n",
    "            train_size=0.8,\n",
    "            shuffle=True\n",
    "        )\n",
    "        x_train = tensor(x_train).float()\n",
    "        x_test = tensor(x_test).float()\n",
    "        y_train = tensor(y_train).float()\n",
    "        y_test = tensor(y_test).float()\n",
    "\n",
    "        train_data_loader = DataLoader(\n",
    "            dataset=FBRelationV2PreprocessedDataset(x_train, y_train),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        valid_data_loader = DataLoader(\n",
    "            dataset=FBRelationV2PreprocessedDataset(x_test, y_test),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        trainer = create_supervised_trainer(\n",
    "            model=self.neural_net,\n",
    "            optimizer=torch.optim.Adam(self.neural_net.parameters(), self.learning_rate),\n",
    "            loss_fn=torch.nn.MSELoss()\n",
    "        )\n",
    "\n",
    "        evaluator = create_supervised_evaluator(\n",
    "            model=self.neural_net,\n",
    "            metrics={\n",
    "                'MSE': Loss(torch.nn.MSELoss())\n",
    "            }\n",
    "        )\n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def log_training_results(trainer):\n",
    "            evaluator.run(train_data_loader)\n",
    "            metrics = evaluator.state.metrics\n",
    "            print(\"Training Results - Epoch: {}. Avg MSE loss: {:.8f}\"\n",
    "                  .format(trainer.state.epoch, metrics['MSE']))\n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def log_validation_results(trainer):\n",
    "            evaluator.run(valid_data_loader)\n",
    "            metrics = evaluator.state.metrics\n",
    "\n",
    "            print(\"Validation Results - Epoch {}. Avg MSE loss: {:.8f}\".format(\n",
    "                trainer.state.epoch,\n",
    "                metrics['MSE']\n",
    "            ))\n",
    "\n",
    "        trainer.run(train_data_loader, max_epochs=self.num_epochs)\n",
    "\n",
    "    def predict(self, features):\n",
    "        features = np.array([feature.likes_preprocessed_v1 for feature in features]).reshape(-1, 2)\n",
    "\n",
    "        test_data_loader = DataLoader(\n",
    "            dataset=FBRelationV2PreprocessedDataset(features, None),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        for batch_idx, (data) in enumerate(test_data_loader):\n",
    "            output = self.neural_net(data)\n",
    "            prediction = int_to_age_category(int(torch.max(output, 0).indices))\n",
    "            self.predictions.append(prediction)\n",
    "\n",
    "        return self.predictions\n",
    "\n",
    "    \n",
    "def pre_process_likes_v2(data_path: str) -> pd.DataFrame:\n",
    "    original_csv_file_path = os.path.join(data_path, 'Relation', 'Relation.csv')\n",
    "    preprocessed_csv_file_path = os.path.join(data_path, 'Relation', 'relation_preprocessed_raw_v2.csv')\n",
    "\n",
    "    if os.path.isfile(preprocessed_csv_file_path):\n",
    "        features = load_likes_csv_file(preprocessed_csv_file_path)\n",
    "    else:\n",
    "        relation_df = load_likes_csv_file(original_csv_file_path)\n",
    "        user_ids = relation_df['user_id'].unique()\n",
    "        users_like_vectors =\n",
    "        like_counts_per_page = relation_df['like_id'].value_counts()\n",
    "        features = relation_df.assign(\n",
    "            user_id=like_counts_per_user.keys(),\n",
    "            likes_given=like_counts_per_user.values\n",
    "        )\n",
    "        features = features.assign(\n",
    "            pages_liked_sum_likes=np.array([\n",
    "                np.array([\n",
    "                    get_page_total_likes(page_id) for page_id in get_page_ids_liked_by_user(relation_df, user_id)\n",
    "                ]).sum()\n",
    "                for user_id in get_user_ids(features)\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    # Standardize features by removing the mean and scaling to unit variance\n",
    "    features[features.columns[1:]] = features[features.columns[1:]].apply(\n",
    "        lambda df: (df-df.mean())/df.std()\n",
    "    ).fillna(0)\n",
    "\n",
    "    return features\n",
    "    \n",
    "def create_common_like_ids(data_path):\n",
    "    relation_df_train = pd.read_csv(os.path.join(data_path, 'Train/Relation/Relation.csv')).drop(columns=['Unnamed: 0'])\n",
    "    relation_df_test = pd.read_csv(os.path.join(data_path, 'Public_Test/Relation/Relation.csv')).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "    train_set_like_ids = set(relation_df_train['like_id'])\n",
    "    test_set_like_ids = set(relation_df_test['like_id'])\n",
    "    common_like_ids = set(sorted(train_set_like_ids.intersection(test_set_like_ids)))\n",
    "    \n",
    "    \n",
    "    \n",
    "def read_training_data():\n",
    "    # page ids that exist both in the training data and test data\n",
    "    common_like_ids = np.genfromtxt('data/relation_v2_common_like_ids.csv', delimiter=',')\n",
    "    \n",
    "    pre_process_likes_v2(\n",
    "        '../../new_data/Train/Relation/Relation.csv'\n",
    "        '../../new_data/Public_Test/Train/Relation/Relation.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_common_like_ids('../../new_data/')\n",
    "\n",
    "# # =============\n",
    "# # main function\n",
    "# # =============\n",
    "# features, labels = read_training_data()\n",
    "# #features = read_prediction_data()\n",
    "\n",
    "# relation_age_estimator_v2 = RelationV2AgeEstimator()\n",
    "# relation_age_estimator_v2.fit(features, labels)\n",
    "# #relation_age_estimator_v2.predict(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
